---
layout: post
title: "generative ai"
date: 2022-09-27
categories:
---
## pivoting
After deciding to move on from our scripts to internal apps idea, we were deeply considering presenting on spreadsheets at demo day. After a minor team crisis on how to throw together a presentation for demo day, we ultimately decided to pull out of presenting. Around the same time, we became more enamored with the Twitter/VC buzz around generative AI and, as a team, decided that it was worth a deep dive exploration.


## recent hype
Right now is an extremely exciting time for generative AI. With the release of GPT-3 last year, the public's eyes were opened to the possibility of AI surpassing human ability for types of artistic work. And, with the recent release of DALL-E 2 and the even more recent release of Whisper by Open AI, the possibility has turned into reality. The craziest things is that the models that these AI's are built on, the transformer, is relatively simple/unimaginative. We just simply threw more data at these models, and the results have been stupidly good—with how fast the field is moving, it almost seems like an inevitablity that AI will far surpass anyone's expectations as more thought/development is sunk into this space.

I've been reading a lot of content in this space over the past couple days, and I don't think it's time-efficient to run through each piece. Instead, here are some of my thoughts on just a few that I've picked.

## solving math olympiad
OpenAI publishes [paper](https://openai.com/blog/formal-math/) showing that they can solve some basic math olympiad problems. The language model isn't achieving a perfect score on the 2023 IMO, but the fact that it can write mathematically correct proofs honestly rattled me. As some of you guys know, training/competing in the US Math/Physics Olympiads were a huge part of my life growing up. Starting from the 5th/6th grade, I estimate I averaged around 5 hours a day on solely prepping for these competitions. As toxic/elitist as my mindset was, I did attribute my own worth/intelligence to how well I could perform on these competitions—in my head, it's what made me special, and it was almost the sole metric that I judged my peers on. The fact that a large language model is so close to essentially rendering my entire childhood pointless is jarring to say the least. I put this article here to highlight what people will most likely feel when AI starts replacing entire jobs—AI is getting so good so quickly that massive job displacement seems almost inevitable. AI companies will be able to do what humans currently do, but faster, cheaper, and more accurately. I'm sure society will adapt, but I expect that the hit to individual human resulting from being replaced by an AI isn't something that'll be apparent to most people until it happens to them.

Digging into the paper itself, solving math problems is an interesting problem due to two main reasons. First, the action space for math proofs is infinite—at each step, the model must choose from a complex and inifinite set of tactics, including mathematical techniques/definitions that have not been discovered yet. Secondly, the typical approach of "self-play" that researchers have historically used to solve games (think when DeepMind solved Go) doesn't work in math because there's no adversary.

These two points are interesting because the paper's initial stab at solving these types of problems is to define a large set of existing strategies and break down the problem into smaller and smaller subproblems until it knows how to solve them. Then, it pieces them together in a series of logical steps to arrive at the final solution. Verifying that the solution is correct is easy (just check each step is logically sound). This is particularly interesting because it's pretty much how I, along with many other kids, used to prep for and solve math competitions problems. You essentially spend all day solving as many problems as you can, ingraining a bunch of tools/techniques. Then, when you're presented with a problem, you try to break it down into more manageable subproblems, and hopefully at that point you've practiced enough that you can figure out which tool best applies to that subproblem. It's what makes the AIME harder than the AMC—each problem is just more complicated, so instead of solving one AMC problem, you have to break apart one AIME problem into several AMC problems, solve those, and piece them back together to get your final answer.

## chinchilla
DeepMind released [Chinchilla](https://www.deepmind.com/publications/an-empirical-analysis-of-compute-optimal-large-language-model-training), a model achieving similar levels of performance to GPT-3 with a fraction of the size. The paper proves that GPT-3 wasn't trained with enough data relative to the complexity of the model (a good analogy here that I've heard is an oversized suitcase with too few items inside). This, combined with scaling laws, seems to suggest that today's models, even with their relatively straightforward transformer architectures, has room to vastly improve if given more data. This is super interesting to think about—the technology might already be there for super powerful AI. All we're missing is more training data. This [article](https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications) is a good read on the implications of Chinchilla if you want more details.

## agi
This is a really interesting [article](https://www.lesswrong.com/posts/K4urTDkBbtNuLivJx/why-i-think-strong-general-ai-is-coming-soon) arguing that AGI is coming soon (median ~2030). While its doomsday predictions are scary, I'm not sure how legitimate of a concern it is. The core points of this article is this—we've already beat everyone's expectations of how much intelligence we can capture versus how much effort is put into AI research. Everything in this space indicates us capturing even more intelligence with a trivial amount of additional effort (aforementioned Chinchilla paper proves we just need to feed more data). The author seems very concerned with what bad actors can do with the power of AGI (i.e. what if you ask an AI to create a ton of deadly diseases). The research in AI ethics has not really caught up, so they worry that if AGI is released soon, we won't have the safety precautions in place to prevent such a doomsday scenario (a single person with the leverage of AI could wreak a lot of havoc).

## rise of large language models
Russell Kaplan, one of our Neo accelerator mentors, published [this interesting twitter thread](https://twitter.com/russelljkaplan/status/1513128007434530818). It's probably easiest if you read it directly, but he essentially details what the world of products with embedded AI will look like. Being able to control large amounts of compute will give those companies the most leverage over everyone else, and they will be able to gatekeep in the same way Google/Apple/Facebook are able to gatekeep companies from their various platforms (and force everyone to optimize around them). This is something that's paramount on my mind as my startup and I are exploring building products with embedded AI (and potentially tools to help other developers/companies embed intelligence into their products).